# OpenNMT train steps

## reference
https://github.com/ymoslem/OpenNMT-Tutorial/tree/main

https://github.com/ymoslem/OpenNMT-Tutorial/blob/main/1-NMT-Data-Processing.ipynb

https://github.com/ymoslem/OpenNMT-Tutorial/blob/main/2-NMT-Training.ipynb

## dataset
https://opus.nlpl.eu/

## environment
- above python 3.8 
- need GPU 


## step

### data process
``` bash
# Create a directory and clone the Github MT-Preparation repository
mkdir -p nmt
cd nmt
git clone https://github.com/ymoslem/MT-Preparation.git     

# Install the requirements
pip3 install -r MT-Preparation/requirements.txt

# Download and unzip a dataset
wget https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip
unzip en-fr.txt.zip

# Filter the dataset
# Arguments: source file, target file, source language, target language
python3 MT-Preparation/filtering/filter.py UN.en-fr.fr UN.en-fr.en fr en

```

### Tokenization / Sub-wording

```bash
# Train a SentencePiece model for subword tokenization

python3 MT-Preparation/subwording/1-train_unigram.py UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en

# Subword the dataset

python3 MT-Preparation/subwording/2-subword.py source.model target.model UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en
     
# First 3 lines before subwording
head -n 3 UN.en-fr.fr-filtered.fr && echo "-----" && head -n 3 UN.en-fr.en-filtered.en
     
# First 3 lines after subwording
head -n 3 UN.en-fr.fr-filtered.fr.subword && echo "---" && head -n 3 UN.en-fr.en-filtered.en.subword
```

### Data Splitting

```bash
# Split the dataset into training set, development set, and test set
# Development and test sets should be between 1000 and 5000 segments (here we chose 2000)

python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 UN.en-fr.fr-filtered.fr.subword UN.en-fr.en-filter


# Line count for the subworded train, dev, test datatest
wc -l *.subword.*

# Check the first and last line from each dataset

# -------------------------------------------
# Change this cell to print your name
echo -e "My name is: FirstName SecondName \n"
# -------------------------------------------

!echo "---First line---"
head -n 1 *.{train,dev,test}

!echo -e "\n---Last line---"
tail -n 1 *.{train,dev,test}
```
 
## train

### Install OpenNMT-py 3.x
`pip3 install OpenNMT-py`

### prepare your datasets

### Create the Training Configuration File

- train_steps - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option early_stopping can help stop the training when there is no considerable improvement.
- valid_steps - 10000 can be good if the value train_steps is big enough.
- warmup_steps - obviously, its value must be less than train_steps. Try 4000 and 8000 values.

```yaml
## Where the samples will be written
save_data: run

# Training files
data:
    corpus_1:
        path_src: UN.en-fr.fr-filtered.fr.subword.train
        path_tgt: UN.en-fr.en-filtered.en.subword.train
        transforms: [filtertoolong]
    valid:
        path_src: UN.en-fr.fr-filtered.fr.subword.dev
        path_tgt: UN.en-fr.en-filtered.en.subword.dev
        transforms: [filtertoolong]

# Vocabulary files, generated by onmt_build_vocab
src_vocab: run/source.vocab
tgt_vocab: run/target.vocab

# Vocabulary size - should be the same as in sentence piece
src_vocab_size: 50000
tgt_vocab_size: 50000

# Filter out source/target longer than n if [filtertoolong] enabled
src_seq_length: 150
src_seq_length: 150

# Tokenization options
src_subword_model: source.model
tgt_subword_model: target.model

# Where to save the log file and the output models/checkpoints
log_file: train.log
save_model: models/model.fren

# Stop training if it does not imporve after n validations
early_stopping: 4

# Default: 5000 - Save a model checkpoint for each n
save_checkpoint_steps: 1000

# To save space, limit checkpoints to last n
# keep_checkpoint: 3

seed: 3435

# Default: 100000 - Train the model to max n steps 
# Increase to 200000 or more for large datasets
# For fine-tuning, add up the required steps to the original steps
train_steps: 3000

# Default: 10000 - Run validation after n steps
valid_steps: 1000

# Default: 4000 - for large datasets, try up to 8000
warmup_steps: 1000
report_every: 100

# Number of GPUs, and IDs of GPUs
world_size: 1
gpu_ranks: [0]

# Batching
bucket_size: 262144
num_workers: 0  # Default: 2, set to 0 when RAM out of memory
batch_type: "tokens"
batch_size: 4096   # Tokens per batch, change when CUDA out of memory
valid_batch_size: 2048
max_generator_batches: 2
accum_count: [4]
accum_steps: [0]

# Optimization
model_dtype: "fp16"
optim: "adam"
learning_rate: 2
# warmup_steps: 8000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]

```

### Build Vocabulary

```bash

# Find the number of CPUs/cores on the machine
nproc --all
     

# Build Vocabulary
# -config: path to your config.yaml file
# -n_sample: use -1 to build vocabulary on all the segment in the training dataset
# -num_threads: change it to match the number of CPUs to run it faster

onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2

# Check if the GPU is active
!nvidia-smi -L

```

### Check if the GPU is visable to PyTorch
```python



import torch

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

gpu_memory = torch.cuda.mem_get_info(0)
print("Free GPU memory:", gpu_memory[0]/1024**2, "out of:", gpu_memory[1]/1024**2)

```

### tranning

Now, start training your NMT model! ðŸŽ‰ ðŸŽ‰ ðŸŽ‰

```bash

rm -rf drive/MyDrive/nmt/models/
     
# Train the NMT model
onmt_train -config config.yaml


# continue train

onmt_train -config config.yaml -train_from models/model.zhen_step_45000.ptc

```

### Translation

```
# Translate the "subworded" source file of the test dataset
# Change the model name, if needed.
onmt_translate -model models/model.fren_step_3000.pt -src UN.en-fr.fr-filtered.fr.subword.test -output UN.en.translated -gpu 0 -min_length 1

# Check the first 5 lines of the translation file
head -n 5 UN.en.translated


# If needed install/update sentencepiece
pip3 install --upgrade -q sentencepiece

# Desubword the translation file
python3 MT-Preparation/subwording/3-desubword.py target.model UN.en.translated

# Check the first 5 lines of the desubworded translation file
head -n 5 UN.en.translated.desubword


# Desubword the target file (reference) of the test dataset
# Note: You might as well have split files *before* subwording during dataset preperation, 
# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.
!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en-fr.en-filtered.en.subword.test
     
# Check the first 5 lines of the desubworded reference
head -n 5 UN.en-fr.en-filtered.en.subword.test.desubword

```

### MT Evaluation

There are several MT Evaluation metrics such as BLEU, TER, METEOR, COMET, BERTScore, among others.



```bash
# Download the BLEU script
wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py
     

# Install sacrebleu
pip3 install sacrebleu     


# Evaluate the translation (without subwording)
python3 compute-bleu.py UN.en-fr.en-filtered.en.subword.test.desubword UN.en.translated.desubword

```

### more Features and directions to explore.

- Icrease **train_steps** and see to what extent new checkpoints provide better translation, in terms of both BLEU and your human evaluation.

- Check other MT Evaluation mentrics other than BLEU such as TER, WER, METEOR, COMET, and BERTScore. What are the conceptual differences between them? Is there special cases for using a specific metric?
- Continue training from the last model checkpoint using the -train_from option, only if the training stopped and you want to continue it. In this case, train_steps in the config file should be larger than the steps of the last checkpoint you train from.

  `onmt_train -config config.yaml -train_from models/model.fren_step_3000.pt`

- Ensemble Decoding: During translation, instead of adding one model/checkpoint to the -model argument, add multiple checkpoints. For example, try the two last checkpoints. Does it improve quality of translation? Does it affect translation seepd?
- Averaging Models: Try to average multiple models into one model using the average_models.py script, and see how this affects translation quality.

  `python3 average_models.py -models model_step_xxx.pt model_step_yyy.pt -output model_avg.pt`
  
- Release the model: Try this command and see how it reduce the model size.
    
  `onmt_release_model --model "model.pt" --output "model_released.pt`
  
- Use CTranslate2: For efficient translation, consider using CTranslate2, a fast inference engine. Check out an example.

- Publish a demo: Show off your work through a simple demo with CTranslate2 and Streamlit.

  
### CTranslate2

- First convert your OpenNMT-py or OpenNMT-tf model to a CTranslate2 model.

  ` pip3 install ctranslate2`
-  OpenNMT-py:

   `ct2-opennmt-py-converter --model_path model.pt --output_dir enja_ctranslate2 --quantization int8`   
  
  
- run trans.py

```python 

import ctranslate2
import sentencepiece as spm

# Set file paths
source_file_path = "en-test.txt"
target_file_path = "zh-test.txt"

sp_source_model_path = "source.model"
sp_target_model_path = "target.model"

ct_model_path = "enzh_ctranslate2/"


# Load the source SentecePiece model
print("load mode");
sp = spm.SentencePieceProcessor()
sp.load(sp_source_model_path)

# Open the source file
print("read source file")
with open(source_file_path, "r") as source:
  lines = source.readlines()

print("line [0]:\t" + lines[0]);
source_sents = [line.strip() for line in lines]
print("source sent[0]:\t" + source_sents[0]);
# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint
# Subword the source sentences
print(" Subword the source sentences");
#source_sents_subworded = sp.encode_as_pieces(source_sents)
source_sents_subworded = sp.encode(source_sents, out_type=str)

# Translate the source sentences
translator = ctranslate2.Translator(ct_model_path, device="cuda")  # or "cuda" for GPU
translations = translator.translate_batch(source_sents_subworded, batch_type="tokens", max_batch_size=4096)
translations = [translation.hypotheses[0] for translation in translations]

# Load the target SentecePiece model
sp.load(sp_target_model_path)

# Desubword the target sentences
translations_desubword = sp.decode(translations)


# Save the translations to the a file
with open(target_file_path, "w+", encoding="utf-8") as target:
  for line in translations_desubword:
    target.write(line.strip() + "\n")

print("Done")

```

## REST Server deploy.

### edit conf.json

```json
{
    "models_root": "./models",
    "models": [
       {
            "id": 102,
            "model": "model.zhen_step_190000.pt",
            "timeout": 6000,
            "on_timeout": "to_cpu",
            "load": true,
            "opt": {
                "gpu": 0,
                "beam_size": 5
            },
	    "tokenizer": {
               "src": {
                "model": "../source.model",
                "type": "sentencepiece"
            },
            "tgt": {
                "model": "../target.model",
                "type": "sentencepiece"
            }
            }
       }


    ]
}

```

### Run Server

- GET Code  https://github.com/OpenNMT/OpenNMT-py.git or use Onmt_server

- Install flask
  `pip install flask`
  
- Start the server

```shell
export IP="0.0.0.0"
export PORT=5000
export URL_ROOT="/translator"
export CONFIG="./available_models/conf.json"

# NOTE that these parameters are optionnal
# here, we explicitely set to default values
python server.py --ip $IP --port $PORT --url_root $URL_ROOT --config $CONFIG
```  








